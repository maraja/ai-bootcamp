{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:450px;\" src=\"https://durhamcollege.ca/wp-content/uploads/ai-hub-header.jpg\" alt=\"DC Logo\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON 7 - Perceptrons, Logistic Regression, and SVMs\n",
    "\n",
    "## <span style=\"color: green\">OVERVIEW</span>\n",
    "In this post we’ll talk about one of the most fundamental machine learning algorithms: the perceptron algorithm. This algorithm forms the basis for many modern day ML algorithms, most notably neural networks. In addition, we’ll discuss the perceptron algorithm’s cousin, logistic regression. And then we’ll conclude with an introduction to SVMs, or support vector machines, which are perhaps one of the most flexible algorithms used today.\n",
    "\n",
    "This lesson, <b>restructured from the first in a series of ML tutorials created by Daniel Geng & Shannon Shih (https://ml.berkeley.edu/blog/2016/11/06/tutorial-2/)</b>, aims to make machine learning accessible to anyone willing to learn. They’ve designed it to give you a solid understanding of how ML algorithms work as well as provide you the knowledge to harness it in your projects.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "<hr/>\n",
    "<br/><br/>\n",
    "\n",
    "## <span style=\"color: green\">Section 2 - Supervised and Unsupervised Algorithms</span>\n",
    "\n",
    "In machine learning, there are two general classes of algorithms. You’ll remember that in our last post we discussed regression and classification. These two methods fall under the larger umbrella of supervised learning algorithms, one of two classes of machine learning algorithms. The other class of algorithms is called unsupervised algorithms.\n",
    "\n",
    "<b>Supervised</b> algorithms learn from labeled training data. The algorithms are “supervised” because we know what the correct answer is. If the algorithm receives a bunch of images labeled as apples or oranges it can first guess the object in the image, then use the label to check if its guess is correct.\n",
    "\n",
    "<b>Unsupervised</b> learning is a bit different in that it finds patterns in data. It works similarly to the way we humans observe patterns (or objects) in random phenomena. Unsupervised learning algorithms do the same thing by looking at unlabeled data. Just like we don’t have a particular goal when looking at an object (other than identifying it), the algorithm doesn’t have a particular goal other than inferring patterns from the data itself.\n",
    "\n",
    "We’ll talk about unsupervised algorithms in a later blog post. For now, let’s look at a very simple supervised algorithm, called the <b>perceptron algorithm.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr/>\n",
    "<br/><br/>\n",
    "\n",
    "## <span style=\"color: green\">Section 3 - Perceptrons</span>\n",
    "\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_5.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n",
    "\n",
    "One of the goals of machine learning and AI is to do what humans do and even surpass them. Thus, it makes sense to try and copy what makes humans so great at what they do–their brains.\n",
    "\n",
    "The brain is composed of billions of interconnected neurons that are constantly firing, passing signals from one neuron to another. Together, they allow us to do incredible things such as recognize faces, patterns, and most importantly, think.\n",
    "\n",
    "The job of an individual neuron is simple: if its inputs match certain criteria, it fires, sending a signal to other neurons connected to it. It’s all very black and white. Of course, the actual explanation is much more complicated than this, but since we’re using computers to simulate a brain, we only need to copy the idea of how a brain works.\n",
    "\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_6.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron mimics the function of neurons in machine learning algorithms. The perceptron is one of the oldest machine learning algorithms in existence. When it was first used in 1957 to perform rudimentary image recognition, the New York Times called it:\n",
    "\n",
    ">*the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\n",
    "We’re still a little far off from that, but the Times did recognize the potential of a perceptron. Perceptrons now form the basis for more complicated neural networks, which we’ll be talking about in the next post.*\n",
    "\n",
    "Like neurons, perceptrons take in several inputs and spit out an output. For example, a perceptron might take as input temperature and try to answer the question, “Should I wear a sweater today?” If the input temperature is below a certain threshold (say, 70˚F), the perceptron will output 1 (yes). If the temperature is above the threshold, the perceptron will output a 0 (no).\n",
    "\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_7.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n",
    "\n",
    "Of course, we could consider more variables than just temperature when deciding whether or not to wear a sweater. Like how a biological neuron typically can have more than one input electrical impulse, we can make our perceptrons have multiple inputs. In that case, we’ll have to also weight each input by a certain amount. If we were to use temperature, wind speed, and something completely random like the number of people showering in Hong Kong as our inputs, we would want to use different weights for each input. Temperature would probably have a negative weight, because the lower the temperature the more you should probably wear a sweater. Wind speed should have a positive weight, because the higher the wind speeds the more likely you will need to put on a sweater. And as for the number of people showering in Hong Kong, the weight should probably be zero (unless you normally factor in the number of people taking showers in Hong Kong into your decision making).\n",
    "\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_8.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "But someone from Canada might be very used to the cold, so their threshold for wearing a sweater might be a lower temperature than someone from, say, Australia. To express this, we use a bias to specify the threshold for the Canadian and the Australian. You can think of the bias as a measure of how difficult it is for the perceptron to say ‘yes’ or ‘no’.\n",
    "\n",
    "\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_9.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n",
    "<span style=\"align: middle\">Fig1. - A view of the pulpit rock in Norway.</span>\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_10.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n",
    "The Australian is biased towards hotter temperatures and thus has a higher threshold temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr/>\n",
    "<br/><br/>\n",
    "\n",
    "## <span style=\"color: green\">Section 4 - Linear Regression</span>\n",
    "\n",
    "<img src=\"http://www.turingfinance.com/wp-content/uploads/2014/07/Linear-Regression.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n",
    "\n",
    "Linear regression assumes a linear or straight line relationship between the input variables (X) and the single output variable (y).\n",
    "\n",
    "More specifically, that output (y) can be calculated from a linear combination of the input variables (X). When there is a single input variable, the method is referred to as a simple linear regression.\n",
    "\n",
    "In simple linear regression we can use statistics on the training data to estimate the coefficients required by the model to make predictions on new data.\n",
    "\n",
    "The line for a simple linear regression model can be written as:\n",
    "\n",
    "**\\begin{equation*}y = b0 + b1 * x\n",
    "\\end{equation*}**\n",
    "\n",
    "where b0 and b1 are the coefficients we must estimate from the training data.\n",
    "\n",
    "Once the coefficients are known, we can use this equation to estimate output values for y given new input examples of x.\n",
    "\n",
    "It requires that you calculate statistical properties from the data such as mean, variance and covariance.\n",
    "\n",
    "All the algebra has been taken care of and we are left with some arithmetic to implement to estimate the simple linear regression coefficients.\n",
    "\n",
    "Briefly, we can estimate the coefficients as follows:\n",
    "\n",
    "**\\begin{equation*}B1 = sum((x(i) - mean(x)) * (y(i) - mean(y))) / sum( (x(i) - mean(x))^2 )\n",
    "B0 = mean(y) - B1 * mean(x)\\end{equation*}**\n",
    "\n",
    "where the i refers to the value of the ith value of the input x or output y.\n",
    "\n",
    "Don't worry if this doesn't make too much sense yet, we will go much more in detail in following lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br/><br/>\n",
    "<hr/>\n",
    "<br/><br/>\n",
    "\n",
    "## <span style=\"color: green\">Section 5 - Logistic Regression</span>\n",
    "\n",
    "However, life is not as black and white as perceptrons indicate. There is uncertainty in just about anything, even choosing whether or not to put on a sweater. In reality, we don’t immediately put on a sweater the moment it drops below some predefined temperature. It’s more as if at any temperature we have a certain “chance” of putting on a sweater. Maybe at 45 F somebody will have a 95% chance of putting on a sweater, and at 60 F the same person will have a 30% chance of putting on a sweater.\n",
    "\n",
    "To better model life’s complexities, we use logistic regression to find these probabilities. This involves fitting a logistic curve (like the one below) to our data. To do this, we again use gradient descent to choose the best parameters for the model. That is, we find parameters that minimize some cost function.\n",
    "\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_11.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n",
    "\n",
    "The general form of the logistic model is\n",
    "\n",
    "**\\begin{equation*}h(\\theta) = \\frac{1}{1+e^{ -\\theta^Tx}}\n",
    "\\end{equation*}**\n",
    "\n",
    "where *θ* is a vector of parameters, *x* is the input variables, and *h* is the model probabilities. For more information, we suggest you check out Andrew Ng’s notes on [logistic regression.](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf)\n",
    "\n",
    "Logistic regression and the perceptron algorithm are very similar to each other. It’s common to think of logistic regression as a kind of perceptron algorithm on steroids, in that a logistic model can predict probabilities while a perceptron can only predict yes or no. In fact, taking a logistic model and setting all values less than .5 to zero, and all values above .5 to one gives a very similar result to just the perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr/>\n",
    "<br/><br/>\n",
    "\n",
    "## <span style=\"color: green\">Section 6 - Support Vector Machines</span>\n",
    "\n",
    "Support vector machines, or SVMs for short, are a class of machine learning algorithms that have become incredibly popular in the past few years. They are based on a very intuitive idea. Here, we’ll introduce SVMs and go through the key ideas in the algorithm.\n",
    "\n",
    "### <span style=\"color: blue\">Margins</span>\n",
    "\n",
    "If you remember the section on classification from our last post, we classify data by drawing a line, called a decision boundary, to separate them.\n",
    "\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_1.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n",
    "\n",
    "Once we’ve drawn a decision boundary, we can find the margin for each datapoint. The margin for a single data point is the distance from that data point to the decision boundary.\n",
    "\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_2.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n",
    "\n",
    "In a way, a margin is a way to determine how confident we are in our classification. If a data point has a large margin, and hence is very far away from the decision boundary we can be pretty confident about our prediction. However, if a data point has a very small margin, and is hence very close to the decision boundary then we probably aren’t as sure of our classification.\n",
    "\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_3.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n",
    "\n",
    "Now that we’ve defined margins, we can talk about support vectors. A support vector is a vector from the data point with the smallest margin to the decision boundary. In other words, it’s a vector between that data point that is closest to the decision boundary and the boundary itself. This vector, in fact, any margin, will be perpendicular to the decision boundary because the smallest distance between a point and a line is a perpendicular line.\n",
    "\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/2/image_4.png\" alt=\"\" style=\"width:355px;float:middle;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: blue\">Linear Separability</span>\n",
    "\n",
    "If you’ve played around with the simulation enough, you’ll notice that sometimes the algorithm fails completely. This happens only when the data points aren’t **linearly separable.** Think of it this way: when you have a set of data points which you can’t draw a straight line to separate them with, then you have a **linearly inseparable** dataset, and since it’s impossible to draw a line to split them, then the SVM algorithm fails.\n",
    "\n",
    "So how do we deal with linear inseparability? Turns out we can reformulate our optimization problem. Before, we wanted every single data point to be as far away (to the correct side) from the decision boundary as possible. Now, we’ll allow a data point to stray toward the wrong side of the boundary, but we’ll add a “cost” to that happening (remember cost functions from the last post?). This is something that happens very often in machine learning and is called **regularization.** It basically allows our model to be a bit more flexible when trying to classify the data. The cost of violating the decision boundaries can be as high or as low as we want it to be, and is controlled by something called the **regularization parameter.**\n",
    "\n",
    "Mathematically, we implement regularization by adding a term to our cost function. For instance\n",
    "\n",
    "**\\begin{equation*}C = C(\\theta, x_i) + c\\sum_{i} R_i(x_i)\n",
    "\\end{equation*}**\n",
    "\n",
    "could be the regularized cost function, where C (the cost) is a function of all the parameters and all the training data, R is the regularization (or penalty) for each data point, and c is the regularization parameter. A large c would mean the penalty for violating the decision boundary would be very high, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">Try it for yourself!</span>\n",
    "\n",
    "To try your hand at a computationally intensive simulation, check out [this link.](http://cs.stanford.edu/people/karpathy/svmjs/demo/).\n",
    "\n",
    "**Things to try with the simulator**\n",
    "* Use the linear kernel and set the regularization parameter (C) to 1.0e+6. Put two red data points and two green data points. Can the SVM classify linearly inseperable data? How about for other values of C?\n",
    "\n",
    "* Try using the RBF kernel with different sigmas. Does the RBF kernel overfit or underfit the data?\n",
    "\n",
    "* Try using different degrees polynomial kernels. Does the shape for a second degree polynomial kernel look familiar?\n",
    "\n",
    "**Some more resources:**\n",
    "* https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
    "* http://scikit-learn.org/stable/modules/svm.html\n",
    "* https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
