{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:450px;\" src=\"https://durhamcollege.ca/wp-content/uploads/ai-hub-header.jpg\" alt=\"DC Logo\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON 8 - Polynomial Regression\n",
    "\n",
    "## <span style=\"color: green\">OVERVIEW</span>\n",
    "\n",
    "<hr />\n",
    "\n",
    ">**Section 1:** <a href=\"#Degrees-of-Polynomials\">Degrees of Polynomials</a>\n",
    "\n",
    ">**Section 2:** <a href=\"#L1-vs-L2\">L1 and L2 Normalization</a>\n",
    "\n",
    ">**Section 3:** <a href=\"#L1-and-L2-Regularization\">L1 and L2 Regularization</a>\n",
    "\n",
    ">**Section 4:** <a href=\"#Expanding-your-Understanding\">Expandng your Understanding</a>\n",
    "\n",
    "<hr />\n",
    "\n",
    "In this lesson we will quickly review Polynomial Regression, also sometimes referred to as a form of Multiple-Linear regression. After which you will do some practical work with linear, logistic and polynomial regression in Lesson 9.\n",
    "\n",
    ">In general, polynomial regression is the usage of curved instead of straight lines when analyzing the best fit of a series of data-points. It can be used in multiple different ways but its general purpose is curve fitting.\n",
    "\n",
    "This is achieved using **feature selection** to determine the data-points that are best represented by the use of a **curved as opposed to straight line**. This is something that you will most likely have to assess visually as you are preparing your data.\n",
    "\n",
    "<hr />\n",
    "\n",
    "More formally, these curved lines are referred to as an Nth degree polynomial. \n",
    "\n",
    "**Polynomial regression allows us to analyze data in multiple ways and find patterns that are potentially abstract.**\n",
    "\n",
    ">This abstract data can occasionally increase overall learning and prediction if used in addition to other methods. Which is the general purpose of polynomial regression in machine learning, as it accounts for flexibility and enables more complex regression when used in conjunction with other methods. It is worth noting however that it does not necessarily guarantee better (more accurate) results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:350px;\" src=\"http://api.ning.com/files/lihD20i2jrw163Eb7AL8EhBuX4o2PanX83Vux57D4mDA2lU5*2p1ER22HLa95iwNbs7hqUwkiHpD5AKJIsYPtdiKIJGLivV0/Capture.PNG\" alt=\"Poly and Linear Example\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### <span style=\"color:#27ae60\">Degrees of Polynomials</span>\n",
    "\n",
    "As you will notice after closely observing the image below, you can determine a polynomial by both the sections between curves; As well as, the function used to define the curve.\n",
    "\n",
    "Understanding what this means in terms of processing data is that you need to consider very carefully which data-points in your data-set have correlations in terms of what you are assessing.\n",
    "\n",
    "You can usually determine this by visualizing data in a graph to determine commonalities, this can also be achieved sometimes by simply analyzing the logical progression of the relationships of the data-headers for each data-point you want to interpolate. (Google Graphviz, it's great for that!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"Content/nthdegrees.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<hr />\n",
    "\n",
    "### <span style=\"color:#27ae60\">L1 vs L2</span>\n",
    "\n",
    "**While practicing machine learning, you will most certainly come across a choice of the mysterious L1 vs L2.**\n",
    "\n",
    ">*Usually your choices will equate to the following:*\n",
    "\n",
    ">1) **L1-normalization vs L2-normalization.**\n",
    "\n",
    ">    and\n",
    "\n",
    ">2) **L1-regularization vs L2-regularization.**\n",
    "\n",
    "<hr />\n",
    "\n",
    "### <span style=\"color:#27ae60\">L1 and L2 Normalization - Error Functions</span>\n",
    "\n",
    "Intuitively speaking, since an L2-norm squares the error (increasing by a lot if error > 1), the model will see a much larger error ( e vs e^2 ) than the L1-norm.\n",
    "\n",
    ">**Therefore the L2-norm model is much more sensitive to this example, and adjustments to the model must be made in order to minimize this error.**\n",
    "\n",
    "If this example is an outlier, the model will be adjusted to minimize this single outlier case, at the expense of many other common examples, since the errors of these common examples are small compared to the single outlier case.\n",
    "\n",
    "<hr />\n",
    "<br />\n",
    "\n",
    "**L1 Normalization - Least Absolute Deviations/Errors**\n",
    "> In general it is minimizing the **sum of the absolute differences (S)** between the target value (Yi) and the estimated values (f(xi)):\n",
    "\n",
    "<img src=\"http://www.chioka.in/wp-content/uploads/2013/12/l1-norm-formula.png\" style=\"width:250px;\" />\n",
    "<span style=\"padding-left:300px;\">Loss Function</span>\n",
    "<br />\n",
    "\n",
    "<hr />\n",
    "<br />\n",
    "\n",
    "**L2 Normalization - Least Squares Error**\n",
    "> In general it is minimizing the **sum of the square of the differences (S)** between the target value (Yi) and the estimated values (f(xi):\n",
    "\n",
    "<img src=\"http://www.chioka.in/wp-content/uploads/2013/12/l2-norm-formula.png\" style=\"width:250px;\" />\n",
    "<span style=\"padding-left:300px;\">Loss Function</span>\n",
    "<br />\n",
    "\n",
    "<hr />\n",
    "<br />\n",
    "\n",
    "> **The differences of L1-norm and L2-norm as a loss function can be promptly summarized as follows:**\n",
    "\n",
    "<img src=\"http://www.chioka.in/wp-content/uploads/2013/12/L1-vs-L2-properties-loss-function.png\" style=\"width:350px;\" />\n",
    "\n",
    "<hr />\n",
    "\n",
    "### <span style=\"color:#27ae60\">L1 and L2 Regularization</span>\n",
    "\n",
    "**Regularization is a very important technique in machine learning to prevent over-fitting.**\n",
    "\n",
    "Mathematically speaking, it adds a regularization term in order to prevent the coefficients to fit so perfectly they over-fit.\n",
    ">L2 is the sum of the square of the weights, while L1 is the sum of the weights.\n",
    "\n",
    "A regression model that uses an L1 regularization technique is called **Lasso Regression** and one that uses L2 regularization is called **Ridge Regression**.\n",
    ">*The key difference between these two types of regularization is the penalty term.*\n",
    "\n",
    "<hr />\n",
    "<br />\n",
    "\n",
    "**L1 Regularization - Lasso Regression**\n",
    "\n",
    "(*Least Absolute Shrinkage and Selection Operator*)\n",
    ">Adds the “*absolute value of magnitude*” of the coefficient as a penalty term to the loss function.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*4MlW1d3xszVAGuXiJ1U6Fg.png\" style=\"width:350px;\" />\n",
    "<span style=\"padding-left:300px;\">Cost Function</span>\n",
    "<br />\n",
    "\n",
    "Again, if *lambda* is zero then we will get back OLS (ordinary least squares) whereas a very large value will make the coefficients zero, resulting in under-fitting.\n",
    "\n",
    "<hr />\n",
    "<br />\n",
    "\n",
    "**L2 Regularization - Ridge Regression**\n",
    "\n",
    ">Adds a “*squared magnitude*” of the coefficient as a penalty term to the loss function. In the image below the *highlighted* part represents the L2 regularization element.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*jgWOhDiGjVp-NCSPa5abmg.png\" style=\"width:350px;\" />\n",
    "<span style=\"padding-left:300px;\">Cost Function</span>\n",
    "<br />\n",
    "\n",
    "Here, if *lambda* is zero then we get back OLS (ordinary least squares). However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how *lambda* is chosen. This technique works very well to avoid the over-fitting issue.\n",
    "\n",
    "<hr />\n",
    "<br />\n",
    "\n",
    "It is important to remember that Lasso **shrinks** the less important feature’s coefficient to **zero** thus, removing some features altogether. So, this works well for **feature selection** in the case that we have a **large number of features**.\n",
    "\n",
    "Traditional methods like cross-validation, use stepwise regression to handle over-fitting and perform feature selection. They work well with a small set of features but these techniques are also a great alternative when we are dealing with a large set of features, as mentioned above.\n",
    "\n",
    "> **The difference between their properties can be promptly summarized as follows:**\n",
    "\n",
    "<img src=\"http://www.chioka.in/wp-content/uploads/2013/12/L1-vs-L2-properties-regularization.png\" style=\"width:350px;\" />\n",
    "\n",
    "<hr />\n",
    "\n",
    "### <span style=\"color:#27ae60\">Expanding your Understanding</span>\n",
    "\n",
    "1. Why does the use of L2-norm require more attention than L1-norm when preparing data?\n",
    "\n",
    "2. How does a large number of features benefit from regularization?\n",
    "\n",
    "3. When dealing with ordinary least squares, occasional large numbers can result in what?\n",
    "\n",
    "4. What are a few effective measures for handling outlier values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Briefly answer each of the questions posed above to the best of your ability\n",
    "\n",
    "#1\n",
    "\n",
    "#2\n",
    "\n",
    "#3\n",
    "\n",
    "#4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#27ae60\">Practical Application</span>\n",
    "\n",
    "You will find a practical application of polynomial regression at the end of the next lesson (9).\n",
    "\n",
    "Which should give you a better understanding of how it fits into the process of data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
