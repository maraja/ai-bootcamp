{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:450px;\" src=\"https://durhamcollege.ca/wp-content/uploads/ai-hub-header.jpg\" alt=\"DC Logo\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON 11 - Decision Trees\n",
    "\n",
    "## <span style=\"color: green\">OVERVIEW</span>\n",
    "\n",
    "- ref: http://benalexkeen.com/decision-tree-classifier-in-python-using-scikit-learn/\n",
    "- dataset: https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "<hr />\n",
    "\n",
    ">**Step 1:** <a href=\"#Step-1:-Data-Pre-processing\">Data Pre-processing</a>\n",
    "\n",
    ">**Step 2:** <a href=\"#Step-2:-Create-the-model\">Create the Model</a>\n",
    "\n",
    ">**Step 3:** <a href=\"#Step-3:-Train-the-model\">Train the Model</a>\n",
    "\n",
    ">**Step 4:** <a href=\"#Step-4:-Test-the-model\">Test the Model</a>\n",
    "\n",
    "<br />\n",
    "<hr />\n",
    "\n",
    "### <span style=\"color: blue\">Decision Trees</span>\n",
    "\n",
    "Decision Trees can be used as classifier or regression models.\n",
    "\n",
    "In general, a tree structure is constructed that breaks the dataset down into smaller subsets eventually resulting in a prediction. There are decision nodes that partition the data and leaf nodes that provide the prediction that can be followed by traversing simple IF..AND..AND..THEN logic down the nodes.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png\"/>\n",
    "\n",
    "<hr />\n",
    "\n",
    "The root node (aka.the first decision node) partitions the data based on the most influential feature.\n",
    "\n",
    "There are 2 measures for this, <b>Entropy</b> and <b>Gini Impurity</b>.\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Entropy\n",
    "The root node (the first decision node) partitions the data using the feature that provides the most information gain.\n",
    "\n",
    "Information gain tells us how important a given attribute of the feature vectors is.\n",
    "\n",
    "<b>It is calculated as:</b>\n",
    "\n",
    ">$$\\text{Information Gain} = \\text{entropy(parent)} – \\text{[average entropy(children)]}$$\n",
    "\n",
    "<b>Where entropy is a common measure of target class impurity, given as:</b>\n",
    "\n",
    ">$$Entropy = \\Sigma_i – p_i \\log_2 p_i$$\n",
    "\n",
    "><i>Where i is each of the target classes.</i>\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Gini Impurity\n",
    "Gini Impurity is another measure of impurity and is calculated as follows:\n",
    "\n",
    ">$$Gini = 1 – \\Sigma_i p_i^2$$\n",
    "><i>Where i is each of the target classes.</i>\n",
    "\n",
    "Gini impurity is computationally faster as it doesn’t require calculating logarithmic functions.\n",
    "Though in reality which of the two methods is used rarely makes too much of a difference.\n",
    "\n",
    "### Predicting Survival in the Titanic Data Set\n",
    "We’ll be using a decision tree to make predictions about the Titanic data set from Kaggle. This data set provides information on the Titanic passengers and can be used to predict whether a passenger survived or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: <span style=\"color:#27ae60\">Data Pre-processing</span>\n",
    "\n",
    ">**Recall:** Data pre-processing is the step that comes before creating, training, and testing our model by cleaning the data and preparing it for consumption by our model. No model is a good model without only the best data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv', index_col='PassengerId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lets take a look at the data-frame we just created so that we can select the attributes we would like to use for our refined classification model (a Decision Tree).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We will be using Pclass, Sex, Age, SibSp (Siblings aboard), Parch (Parents/children aboard), and Fare to predict whether a passenger survived.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go ahead and re-assign the dataframe we created to only include the features listed above\n",
    "# e.g. if we wanted only Sex, we'd do something like df = df[['Sex']]\n",
    "\n",
    "# type your code here\n",
    "df = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']]\n",
    "\n",
    "# print out the first 5 items of the dataframe to make sure we're on the right track!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We need to convert ‘Sex’ into an integer value of 0 or 1.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use pandas' built in map function to turn all the 'male' instances to 0 and all the 'female' instances to 1\n",
    "# e.g. if you were to do this for handedness, it would look something like:\n",
    "#      df['handedness'] = df['handedness'].map({ 'right': 0, 'left': 1 })\n",
    "\n",
    "# type your code here\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# print out the first 5 items of the dataframe to make sure we're on the right track!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We will also drop any rows with missing values.</b>\n",
    "\n",
    ">Missing values are bad as they tend to screw up our classifier. We don't want anything getting in the way of our super talented model!\n",
    "\n",
    "The data (X) is all our data, and the target (y) is the corresponding result for each row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "Now, we're going to want to split our dataset into training and testing instances. Remember, for both training and testing, we need data and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: <span style=\"color:#27ae60\">Create the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize our model so we can take a look at it's attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the model attributes\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr/>\n",
    "\n",
    "### Step 3: <span style=\"color:#27ae60\">Train the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some of the attributes like *max_depth*, *max_leaf_nodes*, *min_impurity_split*, and *min_samples_leaf* can help prevent overfitting the model to the training data.\n",
    "\n",
    "<i>First we fit our model using our training data.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the training data to the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: <span style=\"color:#27ae60\">Test the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Then we score the predicted output from the model on our test data against our ground truth test data.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We see an accuracy score of ~81.01%, which is significantly better than 50/50 guessing.\n",
    "\n",
    "Let’s also take a look at our confusion matrix:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pd.DataFrame(\n",
    "            confusion_matrix(y_test, y_predict),\n",
    "            columns=['Predicted Not Survival', 'Predicted Survival'],\n",
    "            index=['True Not Survival', 'True Survival']\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL\n",
    "Download the Graphviz tool to visualize the data.\n",
    "https://graphviz.gitlab.io/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tree.export_graphviz(model.tree_, out_file='tree.dot', feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then convert this dot file to a png file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from subprocess import call\n",
    "\n",
    "#call(['dot', '-T', 'png', 'tree.dot', '-o', 'tree.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then view our tree, which will look something similar this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://benalexkeen.com/wp-content/uploads/2017/05/tree.png\" alt=\"Decision Tree\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root node, with the most information gain, tells us that the biggest factor in determining survival is Sex.\n",
    "\n",
    "If we zoom in on some of the leaf nodes, we can follow some of the decisions down.\n",
    "\n",
    "We have already zoomed into the part of the decision tree that describes males, with a ticket lower than first class, that are under the age of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://benalexkeen.com/wp-content/uploads/2017/05/tree_leaf_node.png\" alt=\"Leaf Node\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impurity is the measure as given at the top by Gini, the samples are the number of observations remaining to classify and the value is the how many samples are in class 0 (Did not survive) and how many samples are in class 1 (Survived).\n",
    "\n",
    "<hr />\n",
    "\n",
    "<b>Let’s follow this part of the tree down, the nodes to the left are True and the nodes to the right are False:</b>\n",
    "\n",
    "1. We see that we have 19 observations left to classify: 9 did not survive and 10 did.\n",
    "2. From this point the most information gain is how many siblings (SibSp) were aboard.\n",
    "\n",
    "        A. 9 out of the 10 samples with less than 2.5 siblings survived.\n",
    "        B. This leaves 10 observations left, 9 did not survive and 1 did.\n",
    "    \n",
    "3. 6 of these children that only had one parent (Parch) aboard did not survive.\n",
    "4. None of the children aged > 3.5 survived\n",
    "5. Of the 2 remaining children, the one with > 4.5 siblings did not survive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
